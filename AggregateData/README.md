This folder contains three items: two folders containing multiple different data files and a Python script that is used to aggregate these multiple data files into one file that contains all of the data. The data files used to create the final data file include things such as box office numbers, critic reviews, release dates, and sentiment scores. All of these different types of data were aggregated into the two final csv files using the script found in this folder. The data was added to these files in a format that was easy to use for further steps of the prediction process. An in depth explanation of the data collection process can be found below.

## Data Collection Process

In order to create an initial list of movies to collect data for we used the feature of Metacritic that allowed us to specify the year and a threshold score. By doing this search we got a list of 2016 and 2017 movies that had a Metacritic score of 65 or higher. We were able to get these movies by scraping different HTML pages on Metacritic’s website. This initial filtering process yielded a list of 336 movies from 2016 and a list of 383 movies from 2017. The remaining data was collected using these lists of movie titles. An explanation of how each type of data was collected can be seen below. 

### Twitter Data
To collect the necessary data, the GetOldTweets-Python library was utilized. Originally, the intention was to utilize Internet Archive’s collection of tweets. This collection featured several million tweets per day from Twitter. Utilizing this data source, however, resulted in multiple challenges. First, the data from each day of collection required between one and two gigabytes of storage. Considering that we hoped to gather this Twitter data from every day in 2016 (our training set) and 2017 (our test set), the resulting memory requirement would likely be over a terabyte. Despite gaining access to extra storage through our professor's lab machine, this amount of memory was extreme. In addition, very few of these tweets actually concerned the movies we were intending to gather data on, meaning much of this space would be wasted. Further, while the data contained several million tweets per day, it was not comprised of every tweet from that day. In fact, this dataset contained only a fraction of the hundred of millions of tweets a day, meaning we would miss most of the tweets we were searching for. We then found out about the GetOldTweets-Python library, which we explored, and decided would be a better fit for our task. This library allowed us to search for tweets containing a specific search term between any two dates. A shortcoming of Twitter’s API is that the Standard Tier includes access to only tweets from the most recent 7 days. Since we needed data from 2016 and 2017, this restrictiveness made the API unusable. In contrast, GetOldTweets-Python allowed access to much further back dates, ensuring that we could gather data on movies released in recent years. 
Another challenge faced in data collection was in determining the search term for each movie. While simply looking for a movie’s title would work for most movies, movies with more common names posed problems. For example, the title of the movie Get Out, a best picture nominee in 2017, is simply a common phrase. Searching for the term “get out” results in some tweets discussing the movie, but the vast majority are using it in an unrelated fashion. Consequently, searching for this term would lead to a misleadingly high number of tweets associated with Get Out. To solve this problem, we decided to search for a movie’s official hashtag to better filter out unrelated tweets.
To collect the training data set, any tweet from March 1, 2016 to March 1, 2017 containing a hashtag associated with one of the 336 2016 movies was collected. An initial decision faced was whether to collect Twitter data starting from each movie’s release date, or from immediately after the previous Oscars for every movie. It was decided that we would collect data starting from the previous years Oscars, in case we needed to use Twitter information to determine a movie’s pre-release popularity. In the end, however, this pre-release information was not factored into any of our models. To gain the tweet count data, the total number of tweets for each movie was calculated for each day from this data. This tweet count data was stored such that each movie had a set of 365 tweet count values associated with it, one for every day of data collection. To obtain tweet sentiment data, the Python module TextBlob was utilized. Given a text input, TextBlob returns a score between -1 (bad/sad) and 1 (good/happy) representing the detected sentiment of the tweet. The sentiment of each tweet was analyzed using this method, and a per day average sentiment was calculated for each movie. A challenge faced was performing this analysis in a reasonable amount of time. Initially, the intention was to perform the sentiment analysis on each tweet for a movie, and then take the average sentiment per day and write it to one file with every movie and its respective tweet sentiments. However, performing this analysis for one movie at a time took around 20-30 minutes per movie, meaning that for the 336 2016 movies and 383 2017 movies, it would take over two weeks to complete. Taking this approach would have significantly slowed our group’s progress. As a solution, we took a MapReduce-inspired approach. Instead of performing the analysis sequentially and writing to the same, one file, we simultaneously ran the analysis function on every movie and wrote every movie’s results to its own file. After the analysis had completed, another function wrote the contents from all these different, movie-specific files to one master file that now contained every movie’s average per day tweet sentiment. Now instead of taking two weeks, this process took about 30 minutes, so essentially the time it took to run the analysis for one movie.
On their own, this set of tweet counts and average tweet sentiments for every day for each movie was not necessarily useful. All tweet count would indicate was which movies were most popular, but these are not necessarily the movies that the Academy will like best. To extract a meaningful metric from this data, a ratio was calculated to determine a movie’s “buzz” immediately before the Oscar nominations were announced. Since the Oscars are the last major movie award show of the season, many award shows precede it. Conveniently, the announcement of Oscar nominations generally come during a very active time during movie awards show season, where there is much discussion on the possible nominees and winners for different award shows. The intention of this ratio was to determine how much a movie was being discussed during this time, with the hope that this would indicate the likelihood of a movie being an Oscar best picture candidate. For tweet count, the ratio was calculated by dividing the average number of tweets per day about a movie in the week before the announcement of Oscar nominations by the average number of tweets per day about the same movie during its week of release. By dividing by the average number of tweets during the movie’s week of release, the data was essentially normalized. We did not care if a movie had a lot of tweets about it at all times; all this indicated was popularity, which is not necessarily related to award season success. Instead, we wanted to identify movies that were being talked about more during the week before the Oscar nominations announcement than during their initial week of release. It would be expected that a movie would be most discussed and relevant during its week of release; for it to be more discussed in the lead up to the Oscar nominations announcement could mean that a movie was in fact a likely candidate for the Oscars. The same ratio was calculated for tweet sentiment in order to determine if popular sentiment about a movie was higher during the awards season, possibly indicating that a movie was having a successful awards season, and could be an Oscar contender.

### Box Office Numbers
A website called Box Office Mojo was used to collect the lifetime gross of the movie and the number of theaters the movie showed in. There was no good way to scrape this website for the desired information so the numbers for each movie were entered into a box office number text file manually. If no data was available for a given movie on Box Office Mojo then the value NULL was used for both data entries. These NULL values were dealt with once the data was fed into the different processing algorithms. For lifetime gross, the number was very large compared to others, so to normalize this value we divided by one billion for the genetic algorithm and one hundred million for the random forest classifier. This is necessary especially for the genetic algorithm as the mutations and initial random selection of constants have the same random selection range for every factor. If we did not do this normalization, Disney and Marvel movies, which tend to dominate the box office would have scores that are almost entirely based on box office numbers and nothing else. This could either give them extremely high scores or extremely low scores based on our movie scoring algorithm.

### Metacritic Scores
We used a user built Metacritic API by maracalencc in combination with a Python script to gather both average user review scores and average critic review scores. In the event that the API could not find a user/critic review score, a value of NULL was used for the two entries.

### Release Dates
We used the same Metacritic API and Python script mentioned above to gather release dates for each movie. In the genetic algorithm, this date is converted to a numeric value which is computed as the number of days since January 1 of the Oscar year divided by 365 to normalize. For the random forest classifier, the month number was used instead (i.e. 12 for December, 11 for November, etc.) because the classifier does not work as well on continuous data.

### Previous Winners and Nominees
After collecting all of the data from the sources mentioned above the data was aggregated into a CSV file containing the final formatted data from all sources. In order to account for the data from the other award shows two more entries were added to the CSV and were used to indicate which of the movies had been nominated and won the Critics’ Choice Movie Awards and the Golden Globes. If the movie had been nominated for one of these awards then a one was put in that spot for that movie otherwise a zero was added. 

